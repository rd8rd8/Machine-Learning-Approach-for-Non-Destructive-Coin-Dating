{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import itertools\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_absolute_error,mean_squared_error, r2_score\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "import numpy as np\n",
    "run=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_uvvis=pd.read_csv('dados.csv',index_col='Unnamed: 0').drop(['Material','group'],axis=1)\n",
    "df_xrf=pd.read_csv('dados_xrf.csv',index_col='Unnamed: 0')\n",
    "df_uvvis_teste=pd.read_csv('dados_teste.csv',index_col='Unnamed: 0').drop(['Material','group'],axis=1)\n",
    "df_xrf_teste=pd.read_csv('dados_xrf_teste.csv',index_col='Unnamed: 0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "waves=pd.read_csv('waves.csv')['Wavelength (nm)'].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "group_mass=['Mass']\n",
    "group_ref=[\"ref_val_\"+str(x) for x in range(3648)]\n",
    "group_col=[\"col_val_\"+str(x) for x in range(3648)]\n",
    "group_col_cord=[\"X\",\"Y\",\"Z\"]\n",
    "group_UV_VIS=group_ref+group_col_cord#+group_col\n",
    "elements=['Mg', 'Al', 'Si', 'P ', 'S ', 'Cl', 'K ', 'Ca', 'Ti', 'V ', 'Cr',\n",
    "       'Mn', 'Fe', 'Co', 'Ni', 'Cu', 'Zn', 'As', 'Se', 'Rb', 'Sr', 'Y ',\n",
    "       'Zr', 'Nb', 'Mo', 'Ag', 'Cd', 'Sn', 'Sb', 'Ba', 'La', 'Ce', 'Pr',\n",
    "       'Nd', 'W ', 'Au', 'Hg', 'Pb', 'Bi', 'Th', 'U ', 'LE']\n",
    "group_concentration_xrf=[e.strip()+' Concentration'for e in elements]\n",
    "group_error_xrf=[e.strip()+' Error1s'for e in elements]\n",
    "group_xrf=list(itertools.chain(*zip([e.strip()+' Concentration'for e in elements],[e.strip()+' Error1s'for e in elements])))\n",
    "group_year=['Year']\n",
    "group_sec=['Seculo']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.concat([df_xrf,df_uvvis],axis=1)\n",
    "df_teste=pd.concat([df_xrf_teste,df_uvvis_teste],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.concat([df_xrf,df_uvvis],axis=1)\n",
    "df=df.drop(group_col,axis=1)\n",
    "df_teste=pd.concat([df_xrf_teste,df_uvvis_teste],axis=1)\n",
    "df_teste=df_teste.drop(group_col,axis=1)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_total=pd.concat([df,df_teste],axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_total['Year'][df_total['Year']%100==0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_inv = (\n",
    "#     pd.Series(grouped_indices).explode()                       # turn each list into its own row\n",
    "#      .reset_index(name='Sample')      # bring the list-name into a column, call the values 'Sample'\n",
    "#      .rename(columns={'index':'Concentration'})  # rename the old index\n",
    "#      .set_index('Sample')             # make each sample ID the new index\n",
    "# )\n",
    "# df = df.join(df_inv)\n",
    "# df.rename(columns={'Concentration':'Material_XRF'},inplace=True)\n",
    "# df['Material_XRF']=df['Material_XRF'].str.split(' ').str[0]\n",
    "# waves=pd.read_csv('waves.csv')['Wavelength (nm)'].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelo Séculos - UVVIS + Pessagem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels='all'\n",
    "df_seculos=df.copy()\n",
    "df_seculos_teste=df_teste.copy()\n",
    "df_seculos['Seculo']=((df['Year']//100)*100).astype(str)\n",
    "df_seculos_teste['Seculo']=((df_teste['Year']//100)*100).astype(str)\n",
    "if labels==3:\n",
    "    df_seculos['Seculo'][(df_seculos['Seculo']=='1400')|(df_seculos['Seculo']=='1500')|(df_seculos['Seculo']=='1600')|(df_seculos['Seculo']=='1700')]='<1800'\n",
    "    df_seculos['Seculo'][(df_seculos['Seculo']=='1900')|(df_seculos['Seculo']=='2000')]='>1800'\n",
    "    df_seculos_teste['Seculo'][(df_seculos_teste['Seculo']=='1700')]='<1800'\n",
    "    df_seculos_teste['Seculo'][(df_seculos_teste['Seculo']=='1900')|(df_seculos_teste['Seculo']=='2000')]='>1800'\n",
    "df_seculos.drop(['Year'],axis=1,inplace=True)\n",
    "df_seculos_teste.drop(['Year'],axis=1,inplace=True)\n",
    "df_seculos = df_seculos.replace('<LOD',0)\n",
    "df_seculos_teste = df_seculos_teste.replace('<LOD',0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filling Missing Values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will try different methods to fill the missings values and test each method with a decision tree using cross validation to make sure we select the method that reduces the mean squared error."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following cell, we see if removing the columns full of missing values improves the error of a decision tree or not. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the time, or number of samples, until we get a measure is important to know the type of frequency/wave/gamma that we are working with, we cannot interpolate the missing values, since that will influence or change this important information, so we have two options: either we keep the columns with the missing values and only use machine learning models that allow us to work with missing values or we replace these missing values by 0, because that is the true value - we have missing values because we do not have a measure yet."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "thres=numero_nans/numero_samples\n",
    "\n",
    "Select all columns where the percentage of missing values is less than thres/100 %"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import f1_score, make_scorer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "from sklearn.preprocessing import StandardScaler,MinMaxScaler,RobustScaler,MaxAbsScaler,QuantileTransformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelo Séculos - All Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filling Missing Values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will try different methods to fill the missings values and test each method with a decision tree using cross validation to make sure we select the method that reduces the mean squared error."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following cell, we see if removing the columns full of missing values improves the error of a decision tree or not. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import StratifiedKFold, cross_validate\n",
    "from sklearn.metrics import (\n",
    "    make_scorer, average_precision_score, \n",
    "    roc_auc_score, f1_score, accuracy_score,\n",
    "    precision_recall_curve, auc\n",
    ")\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.utils import resample\n",
    "\n",
    "# --- assume df_seculos and df_seculos_teste already defined ---\n",
    "target = 'Seculo'\n",
    "\n",
    "# 1) Split train/test (you already have X_train, X_test, y_train, y_test)\n",
    "X_train = df_seculos.drop(columns=target)\n",
    "y_train = df_seculos[target]\n",
    "X_test  = df_seculos_teste.drop(columns=target)\n",
    "y_test  = df_seculos_teste[target]\n",
    "\n",
    "# 2) Encode labels\n",
    "lb = LabelEncoder()\n",
    "y_enc_train = lb.fit_transform(y_train)\n",
    "y_enc_test  = lb.transform(y_test)\n",
    "\n",
    "# 3) Over-sample any class with fewer than 5 examples\n",
    "X_over = X_train.copy()\n",
    "y_over = pd.Series(y_enc_train, name='Seculo')\n",
    "classes = np.unique(y_enc_train)\n",
    "\n",
    "for cls in classes:\n",
    "    idx = np.where(y_enc_train == cls)[0]\n",
    "    n_missing = max(0, 5 - len(idx))\n",
    "    if n_missing > 0:\n",
    "        X_min = X_over.iloc[idx]\n",
    "        y_min = y_over.iloc[idx]\n",
    "        X_extra, y_extra = resample(\n",
    "            X_min, y_min,\n",
    "            replace=True, n_samples=n_missing, random_state=42\n",
    "        )\n",
    "        X_over = pd.concat([X_over, X_extra], axis=0)\n",
    "        y_over = pd.concat([y_over, y_extra], axis=0)\n",
    "\n",
    "# === 2) Oversampling de classes pequenas (igual ao seu código) ===\n",
    "X_train_over = X_over.reset_index(drop=True).copy()\n",
    "y_train_over = y_over.reset_index(drop=True).copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# === 2) Scorers ===\n",
    "scoring = {\n",
    "    'au_prc': make_scorer(average_precision_score,response_method=\"predict_proba\", average='macro'),\n",
    "    'roc_auc_ovr': make_scorer(roc_auc_score,response_method=\"predict_proba\", average='macro', multi_class='ovr'),\n",
    "    'f1_macro': make_scorer(f1_score, average='macro'),\n",
    "    'accuracy': make_scorer(accuracy_score)\n",
    "}\n",
    "# 5) Prepare results DataFrame\n",
    "thresholds = [t/100 for t in range(0, 105, 5)]  # 0.0, 0.05, …, 1.0\n",
    "idx = pd.Index(thresholds, name='thres')\n",
    "cols = pd.MultiIndex.from_product([['Nothing','Fill0'], scoring.keys()],\n",
    "                                  names=['impute','metric'])\n",
    "results = pd.DataFrame(index=idx, columns=cols, dtype=float)\n",
    "\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# 6) Loop over thresholds and imputation\n",
    "for thres in thresholds:\n",
    "    # select only columns whose missing-fraction ≤ thres\n",
    "    X_over_=X_over  \n",
    "    mask = (X_over_.isna().sum(axis=0)/X_over_.shape[0] <= thres)\n",
    "    X_sub = X_over_.loc[:, mask]\n",
    "    \n",
    "    for fill, label in [(False,'Nothing'), (True,'Fill0')]:\n",
    "        X_in = X_sub.fillna(0) if fill else X_sub\n",
    "        clf = DecisionTreeClassifier(random_state=42)\n",
    "        cv_res = cross_validate(\n",
    "            clf, X_in, y_over, cv=cv, scoring=scoring, return_train_score=False,error_score='raise'\n",
    "        )\n",
    "        \n",
    "        for metric in scoring:\n",
    "            results.loc[thres, (label, metric)] = cv_res[f'test_{metric}'].mean()\n",
    "\n",
    "display(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import seaborn as sns\n",
    "# import pandas as pd\n",
    "# from sklearn.preprocessing import MinMaxScaler\n",
    "# import matplotlib.pyplot as plt\n",
    "# import numpy as np\n",
    "\n",
    "# # Seleção das colunas desejadas\n",
    "# columns = [('Fill0', str(e)) for e in ['au_prc', 'roc_auc_ovr', 'f1_macro', 'accuracy']]\n",
    "# d = results[columns]\n",
    "\n",
    "# # Min-Max Scaling\n",
    "# scaler = MinMaxScaler()\n",
    "# d_scaled = pd.DataFrame(scaler.fit_transform(d), columns=d.columns, index=d.index)\n",
    "\n",
    "# # Criar anotações no formato \"scaled (real)\"\n",
    "# annot = np.empty_like(d_scaled.values, dtype=object)\n",
    "# for i in range(d_scaled.shape[0]):\n",
    "#     for j in range(d_scaled.shape[1]):\n",
    "#         scaled_val = d_scaled.iloc[i, j]\n",
    "#         real_val = d.iloc[i, j]\n",
    "#         annot[i, j] = f\"{scaled_val:.2f} ({real_val:.3f})\"\n",
    "\n",
    "# # Plot do heatmap\n",
    "# plt.figure(figsize=(10, 6))\n",
    "# sns.heatmap(d_scaled, annot=annot, fmt='', cmap=\"viridis\", cbar=True)\n",
    "# plt.title(\"Heatmap com Valor Escalado e Real (Lado a Lado)\")\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0.15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = (X_train_over.isna().sum() / len(X_train_over)) <= 0.15\n",
    "X_train_over_filled = X_train_over.loc[:, mask].fillna(0)\n",
    "X_test_filled = X_test.loc[:, mask].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_filled.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "from sklearn.preprocessing import StandardScaler,MinMaxScaler,RobustScaler,MaxAbsScaler,QuantileTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import (\n",
    "    StandardScaler, MinMaxScaler, RobustScaler,\n",
    "    MaxAbsScaler, QuantileTransformer, LabelEncoder\n",
    ")\n",
    "from sklearn.model_selection import StratifiedKFold, cross_validate\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import (\n",
    "    make_scorer,average_precision_score, roc_auc_score,\n",
    "    f1_score, accuracy_score, make_scorer\n",
    ")\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# === 1) Scalers a testar ===\n",
    "scalers = {\n",
    "    \"No Scaler\": None,\n",
    "    \"StandardScaler\": StandardScaler(),\n",
    "    \"MinMaxScaler\": MinMaxScaler(),\n",
    "    \"RobustScaler\": RobustScaler(),\n",
    "    \"MaxAbsScaler\": MaxAbsScaler(),\n",
    "    \"QuantileTransformer\": QuantileTransformer(random_state=42)\n",
    "}\n",
    "\n",
    "# === 2) Scorers ===\n",
    "scoring = {\n",
    "    'au_prc': make_scorer(average_precision_score,response_method=\"predict_proba\"),\n",
    "    'roc_auc_ovr': make_scorer(roc_auc_score,response_method=\"predict_proba\", multi_class='ovr'),\n",
    "    'f1_macro': make_scorer(f1_score, average='macro'),\n",
    "    'accuracy': make_scorer(accuracy_score)\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "# === 4) CV setup ===\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# === 5) Loop com Pipeline + cross_validate ===\n",
    "results = pd.DataFrame(index=scalers.keys(), columns=scoring.keys(), dtype=float)\n",
    "\n",
    "for name, scaler in scalers.items():\n",
    "    # montar pipeline:\n",
    "    if scaler is None:\n",
    "        steps = [('clf', SVC(probability=True, random_state=42))]\n",
    "    else:\n",
    "        steps = [('scaler', scaler), ('clf', SVC(probability=True, random_state=42))]\n",
    "    pipe = Pipeline(steps)\n",
    "    \n",
    "    cv_res = cross_validate(\n",
    "        pipe,\n",
    "        X_train_over_filled, y_train_over,\n",
    "        cv=cv,\n",
    "        scoring=scoring,\n",
    "        return_train_score=False\n",
    "    )\n",
    "    # preencher resultados com médias:\n",
    "    for metric in scoring:\n",
    "        # cross_validate retorna chaves tipo 'test_ap_macro', etc.\n",
    "        results.loc[name, metric] = np.mean(cv_res[f'test_{metric}'])\n",
    "\n",
    "# === 6) Exibir ===\n",
    "display(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scal=QuantileTransformer(random_state=42)\n",
    "X_train_over_filled_scal=scal.fit_transform(X_train_over_filled)\n",
    "X_test_filled_scal=scal.transform(X_test_filled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import logging\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.preprocessing import LabelEncoder, QuantileTransformer, OneHotEncoder\n",
    "from sklearn.feature_selection import SelectKBest, f_classif, SelectFromModel, RFE\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from hyperopt import fmin, tpe, Trials, STATUS_OK, space_eval, hp\n",
    "from sklearn.linear_model import LogisticRegression, RidgeClassifier, SGDClassifier\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "from sklearn.gaussian_process.kernels import RBF\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import classification_report, roc_auc_score, confusion_matrix,precision_recall_curve, average_precision_score, auc\n",
    "from sklearn.inspection import permutation_importance\n",
    "from sklearn.metrics import make_scorer,ConfusionMatrixDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.utils import resample\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "import random\n",
    "random.seed(42)\n",
    "# ----------------------\n",
    "# Setup results directory\n",
    "# ----------------------\n",
    "\n",
    "RESULTS_DIR = os.path.join(os.getcwd(), 'results_models')\n",
    "os.makedirs(RESULTS_DIR, exist_ok=True)\n",
    "\n",
    "# ----------------------\n",
    "# Model hyperparameter definitions\n",
    "# ----------------------\n",
    "class models_parameters:\n",
    "#POR DENTRO DE CLASSE\n",
    "    models_params = {\n",
    "                    \"LogisticRegression\": {\n",
    "                            'C': hp.choice('C', [100,250,500,1000,2500,5000]), \\\n",
    "                            #'class_weight': hp.choice('class_weight', [\"balanced\"]),\\\n",
    "                            'l1_ratio': hp.choice('l1_ratio', np.arange(0.1,1,0.1)),\\\n",
    "                            'tol': hp.choice('tol', [1*10**-50,1*10**-25,1*10**-10,1*10**-5,0.001,0.01]),\\\n",
    "                            'warm_start': hp.choice('warm_start', [True, False]),\\\n",
    "                            'fit_intercept': hp.choice('fit_intercept', [True, False]),\\\n",
    "                            # 'penalty':hp.choice('penalty',['elasticnet','l1', 'l2']), # - só com solver saga\n",
    "                            'max_iter': hp.choice('max_iter', [100,250,500,1000]) #100\n",
    "                            },\n",
    "                    \"GaussianProcessClassifier\": {\n",
    "                            'max_iter_predict': hp.choice('max_iter_predict', [5,50,100,500,1000])\n",
    "                            },\n",
    "                    \"RandomForestClassifier\": {\n",
    "                            'n_estimators': hp.choice('n_estimators', [100,150,200,250,500,1000]), #110\n",
    "                            'max_depth': hp.choice('max_depth', [2,3,4,5,6,7,8,9,10]),#3\n",
    "                            'min_samples_split': hp.choice('min_samples_split',range(5,100,5) ), #range(20,80,2)\n",
    "                            'min_samples_leaf': hp.choice('min_samples_leaf', range(10,100,10)), #30\n",
    "                            #'max_features': hp.choice('max_features', range(10,100,10)),\n",
    "                            'max_leaf_nodes': hp.choice('max_leaf_nodes', [2,3,4,5,6,7,8,9,10])#-\n",
    "                            },\n",
    "                    \"GradientBoostingClassifier\": {\n",
    "                            # 'learning_rate': hp.choice('learning_rate', np.arange(0.1,1,0.1)),\n",
    "                            # 'n_estimators': hp.choice('n_estimators', [10,50,100,200,500,1000]),\\\n",
    "                            # 'max_depth': hp.choice('max_depth', [2,3,4,5,6,7,8,9,10]),\\\n",
    "                            # 'min_samples_split': hp.choice('min_samples_split',range(10,1000,100)),\n",
    "                            # 'min_samples_leaf': hp.choice('min_samples_leaf', range(10,1000,100)),\\\n",
    "                            # #'max_features': hp.choice('max_features', [2,4,6,8,10]),\n",
    "                            # 'ccp_alpha': hp.choice('ccp_alpha', np.arange(0,10,0.1)),\\\n",
    "                            # 'max_leaf_nodes': hp.choice('max_leaf_nodes', [2,3,4,5,6,7,8,9,10]),\n",
    "                            # 'tol': hp.choice('tol', [1*10**-50,1*10**-25,1*10**-10,1*10**-5,0.001,0.01])\n",
    "                        # 'learning_rate': hp.choice('learning_rate', np.arange(0.01, 0.99, 0.01)),\n",
    "                        # 'n_estimators': hp.choice('n_estimators', [100,150,200,250,300,350,400,450,500,1000]),\n",
    "                        # 'max_depth': hp.choice('max_depth', [2,3,4,5,6,7,8,9,10]),\n",
    "                        # 'min_samples_split': hp.choice('min_samples_split', range(5,100,5)),\n",
    "                        # 'min_samples_leaf': hp.choice('min_samples_leaf',  range(10,100,10)),\n",
    "                        # 'ccp_alpha': hp.choice('ccp_alpha', np.arange(0, 1, 0.01)),\n",
    "                        # 'max_leaf_nodes': hp.choice('max_leaf_nodes', [None,2,3,4,5,6,7,8,9,10,20,30]),\n",
    "                        # 'tol': hp.choice('tol', [1*10**-10,1*10**-5,1*10**-4, 1*10**-3, 1*10**-2,1*10**-1])\n",
    "                        'learning_rate': hp.uniform('learning_rate', 0.9, 1.0),  # Use uniform distribution for learning rate\n",
    "                        'n_estimators': hp.choice('n_estimators', [100, 150, 200, 250, 300,500]),\n",
    "                        'max_depth': hp.choice('max_depth', [2,3, 4,]),  # Avoid overly deep trees\n",
    "                        'min_samples_split': hp.choice('min_samples_split', [2, 5, 10]),  # Adjust according to dataset size\n",
    "                        'min_samples_leaf': hp.choice('min_samples_leaf', [2, 5, 10]),  # Adjust according to dataset size\n",
    "                        'subsample': hp.uniform('subsample', 0.8, 1.0),  # Fraction of samples used for fitting the trees\n",
    "                        'max_features': hp.choice('max_features', ['sqrt', 'log2', None]),\n",
    "                        'ccp_alpha': hp.choice('ccp_alpha', [0.0]),  # Complexity parameter for Minimal Cost-Complexity Pruning\n",
    "                        'max_leaf_nodes': hp.choice('max_leaf_nodes', [None,2, 10, 20, 30]),\n",
    "                        'tol': hp.uniform('tol', 1e-6, 1e-2),  # Tolerance for stopping criterion\n",
    "\n",
    "                        },\n",
    "                    \"MLPClassifier\": {\n",
    "                            'hidden_layer_sizes':hp.choice('hidden_layer_sizes', [(203,124,3),(203,64,3),(203,240,120,3),(203,128,64,256,3)]),\\\n",
    "                            'solver':hp.choice('solver', ['adam','lbfgs','sgd']),\\\n",
    "                            'alpha': hp.choice('alpha', np.arange(0,1,0.1)),\n",
    "                            'batch_size': hp.choice('batch_size', [10,50,100,500]),#10\n",
    "                            #1*10**-25,0.00001,0.001,0.01,0.1\n",
    "                            #sgd causes error\n",
    "                            # 'learning_rate':hp.choice('learning_rate', ['constant','invscaling', 'adaptive']),\\\n",
    "                            'learning_rate_init': hp.choice('learning_rate_init', np.arange(0.1,1,0.1)),\\\n",
    "                            'max_iter': hp.choice('max_iter', [10,50,100,250,500,1000]),#100\n",
    "                            'tol': hp.choice('tol', [1*10**-25,0.00001,0.001]),\\\n",
    "                            'momentum': hp.choice('momentum', [0.001,0.1,0.5,0.7])\n",
    "                            },\n",
    "                    \"KNeighborsClassifier\": {\n",
    "                            # 'algorithm':hp.choice('algorithm', ['ball_tree']), \\\n",
    "                            'metric':hp.choice(\"metric\", ['minkowski','cosine','euclidean','cityblock']), #\n",
    "                            'n_neighbors':hp.choice('n_neighbors',[10,50,100,200]),\n",
    "                            'p':hp.choice('p',[10,50,100,200]),\n",
    "                            'leaf_size':hp.choice('leaf_size',[10,50,100,200]),\n",
    "                            'weights':hp.choice(\"weights\",['uniform','distance'])\n",
    "                            },\n",
    "                    \"XGB_dart\": {\n",
    "                            'sample_type': hp.choice('sample_type', [\"uniform\", \"weighted\"]),\n",
    "                            'normalize_type': hp.choice('normalize_type', [\"tree\",\"forest\"]),\\\n",
    "                            'rate_drop': hp.choice('rate_drop', np.arange(0,1,0.1)),\n",
    "                            'skip_drop': hp.choice('skip_drop', np.arange(0,1,0.1)),\\\n",
    "                            'learning_rate': hp.choice('learning_rate', np.arange(0.1,1,0.1)),\n",
    "                            'max_depth': hp.choice('max_depth', [2,3,4,5,6,7,8,9,10]),\n",
    "                            'min_child_weight': hp.choice('min_child_weight', np.arange(2,100,10)),\n",
    "                            #'subsample': hp.choice('subsample', [1]), default is the best\\\n",
    "                            'gamma': hp.choice('gamma', np.arange(0,1,0.1)),\n",
    "                            'colsample_bytree': hp.choice('colsample_bytree', np.arange(0.1,1,0.1)),\n",
    "                            'lambda': hp.choice('lambda', np.arange(0.1,1,0.1))\n",
    "\n",
    "                            },\n",
    "                    \"XGB_linear\": {\n",
    "                            'alpha': hp.choice('alpha', np.arange(0, 1, 0.01)), #default is better,0\n",
    "                            'lambda': hp.choice('lambda', np.arange(0, 1, 0.01)),\\\n",
    "                            'updater': hp.choice('updater', [\"shotgun\",\"coord_descent\"]),\n",
    "                            'learning_rate': hp.choice('learning_rate', np.arange(0.01, 0.2, 0.01)),\\\n",
    "                            'feature_selector': hp.choice('feature_selector', [\"cyclic\", \"shuffle\"])\n",
    "                            },\n",
    "                    \"XGB_tree\": {\n",
    "                            'learning_rate': hp.choice('learning_rate', np.arange(0.1,1,0.1)), #0.03\n",
    "                            'max_depth': hp.choice('max_depth', [2,3,4,5,6,7,8,9,10]), #4\n",
    "                            'gamma': hp.choice('gamma', np.arange(0,1,0.1)), #0.6\n",
    "                            'lambda':hp.choice('lambda', np.arange(0,1,0.1)), #0.5\n",
    "                            'min_child_weight': hp.choice('min_child_weight', np.arange(2,100,10)) #30\n",
    "                            },\n",
    "                    \"GNB\":{'var_smoothing': hp.loguniform('var_smoothing', np.log(1e-12), np.log(1e-6))},\n",
    "                    \"SVC\": {\n",
    "                        'gamma': hp.choice('epsilon', [0.0001,0.001,0.1,0.5,0.9]),#0.0001\n",
    "                        'C': hp.choice('C',np.arange(10,3000,100)) #constante apartir do 2800\n",
    "                        },\n",
    "                    \"DT\": {\n",
    "                            'criterion': hp.choice('criterion', ['gini', 'entropy']),\n",
    "                            'max_depth': hp.choice('max_depth', np.arange(1, 20, 1)),  # Maximum depth of the tree\n",
    "                            'min_samples_split': hp.choice('min_samples_split', np.arange(2, 20, 1)),  # Minimum number of samples required to split an internal node\n",
    "                            'min_samples_leaf': hp.choice('min_samples_leaf', np.arange(1, 20, 1)),  # Minimum number of samples required to be at a leaf node\n",
    "                            'max_features': hp.choice('max_features', ['sqrt', 'log2', None]),  # Number of features to consider when looking for the best split\n",
    "                            'splitter': hp.choice('splitter', ['best', 'random']),  # Strategy used to choose the split at each node\n",
    "                        }\n",
    "\n",
    "                            }\n",
    "\n",
    "# ----------------------\n",
    "# Preprocessing\n",
    "# ----------------------\n",
    "def query_and_preprocess(df,df_teste):\n",
    "    target = 'Seculo'\n",
    "    # 1) Split train/test (you already have X_train, X_test, y_train, y_test)\n",
    "    X_train = df.drop(columns=target)\n",
    "    y_train = df[target]\n",
    "    X_test  = df_teste.drop(columns=target)\n",
    "    y_test  = df_teste[target]\n",
    "\n",
    "    # 2) Encode labels\n",
    "    lb = LabelEncoder()\n",
    "    y_enc_train = lb.fit_transform(y_train)\n",
    "    y_enc_test  = lb.transform(y_test)\n",
    "\n",
    "    # 3) Over-sample any class with fewer than 5 examples\n",
    "    X_over = X_train.copy()\n",
    "    y_over = pd.Series(y_enc_train, name='Seculo')\n",
    "    classes = np.unique(y_enc_train)\n",
    "\n",
    "    for cls in classes:\n",
    "        idx = np.where(y_enc_train == cls)[0]\n",
    "        n_missing = max(0, 5 - len(idx))\n",
    "        if n_missing > 0:\n",
    "            X_min = X_over.iloc[idx]\n",
    "            y_min = y_over.iloc[idx]\n",
    "            X_extra, y_extra = resample(\n",
    "                X_min, y_min,\n",
    "                replace=True, n_samples=n_missing, random_state=42\n",
    "            )\n",
    "            X_over = pd.concat([X_over, X_extra], axis=0)\n",
    "            y_over = pd.concat([y_over, y_extra], axis=0)\n",
    "\n",
    "    # === 2) Oversampling de classes pequenas (igual ao seu código) ===\n",
    "    X_train_over = X_over.reset_index(drop=True).copy()\n",
    "    y_train_over = y_over.reset_index(drop=True).copy()\n",
    "\n",
    "    X_train_droped_filled = X_train_over.loc[:, X_train_over.isna().sum(axis=0) / X_train_over.shape[0] <= 0.1].fillna(0).copy()\n",
    "    X_test_droped_filled = X_test.loc[:,X_train_droped_filled.columns].fillna(0).copy()\n",
    "    print(X_train_droped_filled.columns)\n",
    "    \n",
    "    # Scale\n",
    "    scaler = QuantileTransformer(output_distribution='uniform', random_state=42)\n",
    "    X_train_scaled = pd.DataFrame(scaler.fit_transform(X_train_droped_filled), columns=X_train_droped_filled.columns)\n",
    "    X_test_scaled  = pd.DataFrame(scaler.transform(X_test_droped_filled),  columns=X_test_droped_filled.columns)\n",
    "    return X_train_scaled, X_test_scaled, y_train_over, pd.Series(y_enc_test),lb\n",
    "\n",
    "# Hyperopt objective with FS\n",
    "# ----------------------\n",
    "def hyperopt(model_name, X_train, y_train, model_space, max_evals=50):\n",
    "    trials = Trials()\n",
    "    n_feats = X_train.shape[1]\n",
    "    fs_space = hp.choice(f'{model_name}_fs', [\n",
    "        {'name': 'none'},\n",
    "        {'name': 'SelectKBest',       'k': hp.quniform(f'{model_name}_k',      3, n_feats, 1)},\n",
    "        {'name': 'SelectFromModel',   'threshold': hp.uniform(f'{model_name}_t', 0.01, 0.5)}\n",
    "        #,{'name': 'RFE',               'n_features': hp.quniform(f'{model_name}_n', 3, n_feats, 1)}\n",
    "    ])\n",
    "    space = {**model_space, 'fs_method': fs_space}\n",
    "\n",
    "    def objective(params):\n",
    "        fs = params.pop('fs_method')\n",
    "        # base model\n",
    "        model = select_model(model_name, params)\n",
    "        # apply FS\n",
    "        if fs['name']=='none':\n",
    "            estimator = model\n",
    "        else:\n",
    "            if fs['name']=='SelectKBest':\n",
    "                selector = SelectKBest(f_classif,k=int(fs['k']))\n",
    "            elif fs['name']=='SelectFromModel':\n",
    "                selector = SelectFromModel(RandomForestClassifier(n_estimators=500, random_state=42),threshold=int(fs['threshold']))\n",
    "            # else: # RFE\n",
    "            #     selector = RFE(RandomForestClassifier(n_estimators=500, random_state=42),n_features_to_select=int(fs['n_features']))\n",
    "            # guard\n",
    "            try:\n",
    "                sub = selector.fit_transform(X_train, y_train)\n",
    "                if sub.ndim<2 or sub.shape[1]==0:\n",
    "                    return {'loss': 0, 'status': STATUS_OK}\n",
    "            except Exception as ti:\n",
    "                print(ti)\n",
    "                return {'loss': 0, 'status': STATUS_OK}\n",
    "            estimator = Pipeline([('fs', selector),('clf', model)])\n",
    "        # evaluate\n",
    "        try:\n",
    "            scorer=make_scorer(average_precision_score,response_method='predict_proba')\n",
    "            score = cross_val_score(estimator, X_train, y_train, cv=5, scoring=scorer).mean()\n",
    "            return {'loss': -score, 'status': STATUS_OK}\n",
    "        except Exception as ti:\n",
    "            print(ti)\n",
    "            return {'loss': 0, 'status': STATUS_OK}\n",
    "\n",
    "    best = fmin(objective, space, algo=tpe.suggest, max_evals=max_evals, trials=trials)\n",
    "    return space_eval(space, best)\n",
    "\n",
    "# ----------------------\n",
    "# Model factory\n",
    "# ----------------------\n",
    "def select_model(name, params):\n",
    "    if name=='LogisticRegression': return LogisticRegression(**params, random_state=42)\n",
    "    if name=='RandomForestClassifier': return RandomForestClassifier(**params, random_state=42)\n",
    "    if name=='GradientBoostingClassifier': return GradientBoostingClassifier(**params, random_state=42)\n",
    "    if name=='MLPClassifier'    : return MLPClassifier(**params, random_state=42)\n",
    "    if name=='KNeighborsClassifier': return KNeighborsClassifier(**params)\n",
    "    if name.startswith('XGB')  :\n",
    "        booster='gbtree'\n",
    "        if name=='XGB_dart': booster='dart'\n",
    "        if name=='XGB_linear': booster='gblinear'\n",
    "        return xgb.XGBClassifier(**params, booster=booster, random_state=42)\n",
    "    if name=='SVC': return SVC(**params)\n",
    "    if name=='GaussianProcessClassifier': return GaussianProcessClassifier(**params, kernel=RBF(), random_state=42)\n",
    "    if name=='GNB': return GaussianNB(**params)\n",
    "    if name=='DT': return DecisionTreeClassifier(**params,random_state=42)\n",
    "    raise ValueError(f'Unknown model: {name}')\n",
    "\n",
    "# ----------------------\n",
    "# Evaluation & Saving\n",
    "# ----------------------\n",
    "def save_evaluation(estimator, X_train, y_train, X_test, y_test,model_name,mname,lb):\n",
    "    estimator.fit(X_train, y_train)\n",
    "    preds = estimator.predict(X_test)\n",
    "    probas = estimator.predict_proba(X_test)\n",
    "    \n",
    "    # Recover original labels\n",
    "\n",
    "    y_test_labels = lb.inverse_transform(y_test)\n",
    "    preds_labels = lb.inverse_transform(preds)\n",
    "    \n",
    "    # ROC AUC\n",
    "    roc = average_precision_score(np.array(y_test_labels), probas)\n",
    "    print(roc)\n",
    "    # Classification report\n",
    "    rep = classification_report(y_test_labels, preds_labels)\n",
    "    txt = os.path.join(RESULTS_DIR, f'eval_{model_name}_{mname}.txt')\n",
    "    with open(txt, 'w') as f:\n",
    "        f.write(f'Model: {mname}\\nDate: {datetime.now()}\\nAU_PRC: {roc:.4f}\\n')\n",
    "        f.write(rep)\n",
    "    \n",
    "    # Permutation importance (top 20)\n",
    "    r = permutation_importance(estimator, X_test, y_test, n_repeats=30, random_state=123)\n",
    "    idx = np.argsort(r.importances_mean)[-20:]  # get 20 most important\n",
    "    fig, ax = plt.subplots(figsize=(10, 8))\n",
    "    ax.boxplot(r.importances[idx].T, vert=False, labels=X_test.columns[idx])\n",
    "    ax.set_title(f'Permutation Importance - {mname}')\n",
    "    perm_path = os.path.join(RESULTS_DIR, f'perm_{model_name}_{mname}.png')\n",
    "    fig.tight_layout()\n",
    "    fig.savefig(perm_path)\n",
    "    plt.close(fig)\n",
    "    \n",
    "    # Confusion matrix\n",
    "    cm = confusion_matrix(y_test_labels, preds_labels, labels=lb.classes_)\n",
    "    fig2, ax2 = plt.subplots(figsize=(8, 6))\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=lb.classes_)\n",
    "    disp.plot(ax=ax2, cmap='Blues')\n",
    "    cm_path = os.path.join(RESULTS_DIR, f'cm_{model_name}_{mname}.png')\n",
    "    fig2.tight_layout()\n",
    "    fig2.savefig(cm_path)\n",
    "    plt.close(fig2)\n",
    "\n",
    "    return roc\n",
    "\n",
    "\n",
    "\n",
    "# ----------------------\n",
    "# Runner\n",
    "# ----------------------\n",
    "def run_feature_selection_pipeline(df,df_teste,model_names,start=0, max_evals=50):\n",
    "    X_train,X_test,y_train,y_test,lb = query_and_preprocess(df,df_teste)\n",
    "    results={}\n",
    "    for name, space in list(models_parameters.models_params.items())[start:]:\n",
    "        print(name)\n",
    "        best = hyperopt(name, X_train, y_train, space, max_evals)\n",
    "        # separate fs and params\n",
    "        fs = best.pop('fs_method')\n",
    "        params = best.copy()\n",
    "        # build estimator\n",
    "        model = select_model(name, params)\n",
    "        print(fs['name'])\n",
    "        if fs['name']=='none':\n",
    "            estimator = model\n",
    "        else:\n",
    "            if fs['name']=='SelectKBest':\n",
    "                selector = SelectKBest(f_classif, k=int(fs['k']))\n",
    "            elif fs['name']=='SelectFromModel':\n",
    "                selector = SelectFromModel(LogisticRegression(random_state=42), threshold=fs['threshold'])\n",
    "            # else: # RFE\n",
    "            #     selector = RFE(LogisticRegression(random_state=42), n_features_to_select=int(fs['n_features']))\n",
    "            # guard\n",
    "            try:\n",
    "                sub = selector.fit_transform(X_train, y_train)\n",
    "                if sub.ndim<2 or sub.shape[1]==0:\n",
    "                    print(sub.ndim)\n",
    "                    print(sub.shape)\n",
    "                    score=0\n",
    "            except:\n",
    "                score=0\n",
    "            estimator = Pipeline([('fs', selector),('clf', model)])\n",
    "        # evaluate\n",
    "        # try:\n",
    "        scorer=make_scorer(average_precision_score,response_method='predict_proba')\n",
    "        score = cross_val_score(estimator, X_train, y_train, cv=5, scoring=scorer).mean()\n",
    "        # write JSON\n",
    "        py_params = {k: (v.tolist() if isinstance(v, np.ndarray) else int(v) if isinstance(v, np.generic) else v)\n",
    "             for k,v in params.items()}\n",
    "        py_fs     = fs.tolist() if isinstance(fs, np.ndarray) else fs\n",
    "        py_score  = float(score)      if isinstance(score, np.generic) else score\n",
    "\n",
    "        out = {\n",
    "            'params': py_params,\n",
    "            'fs':      py_fs,\n",
    "            'score':   py_score\n",
    "        }\n",
    "\n",
    "        with open(os.path.join(RESULTS_DIR, f'best_{model_name}_{name}.json'), 'w') as f:\n",
    "            json.dump(out, f, indent=2)\n",
    "        # final evaluation\n",
    "        save_evaluation(estimator,X_train,y_train,X_test,y_test,model_name,name,lb)\n",
    "        results[name]=out\n",
    "        # except Exception as e:\n",
    "        #     print(e)\n",
    "        #     score=0\n",
    "        # write JSON\n",
    "        # out={'params':params,'fs':fs,'score':score}\n",
    "        # try:\n",
    "        #     with open(os.path.join(RESULTS_DIR,f'best_{model_name}_{name}.json'),'w') as f: json.dump(out,f,indent=2)\n",
    "        # except:\n",
    "        #     continue\n",
    "        # # final evaluation\n",
    "        # results[name]=out\n",
    "    return results\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start=(3,0,0)\n",
    "g=['uvvis','xrf','uvvis_xrf','uvvis_xrf_mass','xrf_mass']\n",
    "for label in [3,4]:\n",
    "    df_seculos=df.copy().drop(columns=['Material_XRF']) if 'Material_XRF' in df.columns else df.copy()\n",
    "    df_seculos_teste=df_teste.copy()\n",
    "    df_seculos['Seculo']=((df['Year']//100)*100).astype(str)\n",
    "    df_seculos_teste['Seculo']=((df_teste['Year']//100)*100).astype(str)\n",
    "    if label==3:\n",
    "        print(3)\n",
    "        df_seculos['Seculo'][(df_seculos['Seculo']=='1400')|(df_seculos['Seculo']=='1500')|(df_seculos['Seculo']=='1600')|(df_seculos['Seculo']=='1700')]='<1800'\n",
    "        df_seculos['Seculo'][(df_seculos['Seculo']=='1900')|(df_seculos['Seculo']=='2000')]='>1800'\n",
    "        df_seculos_teste['Seculo'][(df_seculos_teste['Seculo']=='1700')]='<1800'\n",
    "        df_seculos_teste['Seculo'][(df_seculos_teste['Seculo']=='1900')|(df_seculos_teste['Seculo']=='2000')]='>1800'\n",
    "    elif label==4:\n",
    "        print(4)\n",
    "        df_seculos.drop(df_seculos['Seculo'][(df_seculos['Seculo']=='1400')|(df_seculos['Seculo']=='1500')|(df_seculos['Seculo']=='1600')].index,axis=0,inplace=True)\n",
    "    df_seculos.drop(['Year'],axis=1,inplace=True)\n",
    "    df_seculos_teste.drop(['Year'],axis=1,inplace=True)\n",
    "    df_seculos = df_seculos.replace('<LOD',0)\n",
    "    df_seculos_teste = df_seculos_teste.replace('<LOD',0)\n",
    "    #a=df_seculos.copy().groupby('Seculo').apply(lambda x: x.sample(n=5, random_state=42)).reset_index(drop=True)\n",
    "\n",
    "    for e,ename in list(zip([group_UV_VIS+group_sec,group_xrf+group_sec,group_UV_VIS+group_xrf+group_sec,group_UV_VIS+group_xrf+group_mass+group_sec,group_xrf+group_mass+group_sec],['uvvis','xrf','uvvis_xrf','uvvis_xrf_mass','xrf_mass'])):\n",
    "    #for e,ename in list(zip([group_xrf+group_sec,group_xrf+group_mass+group_sec],['xrf','xrf_mass'])):\n",
    "        model_name=f'{str(label)}labels_{ename}'\n",
    "        if label==start[0] and ename == g[start[1]]:\n",
    "            a=run_feature_selection_pipeline(df_seculos[e],df_seculos_teste[e],model_name,start[2], max_evals=50) #final train and then save_evaluation\n",
    "        elif label==start[0] and ename in g[start[1]+1:]:\n",
    "            run_feature_selection_pipeline(df_seculos[e],df_seculos_teste[e],model_name, max_evals=50)\n",
    "        elif label>start[0]:\n",
    "            run_feature_selection_pipeline(df_seculos[e],df_seculos_teste[e],model_name, max_evals=50)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
